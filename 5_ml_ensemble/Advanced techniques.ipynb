{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced machine learning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From previous lectures you have already learned about different classification algorithms, and also learned how to correctly validate and evaluate the quality of the model. But what if you have already found the best model and improve the accuracy of the model can no longer? In this case, you need to apply more advanced techniques of machine learning, which can be combined with the word \"ensembles\". An ensemble is a certain aggregate, parts of which form a single whole. From everyday life you know musical ensembles, where several musical instruments are united, architectural ensembles with different buildings, etc.\n",
    "\n",
    "## Ensembles\n",
    "\n",
    "A good example of ensembles is Condorcet's theorem \"on the jury of juries\" (1784). If each member of the jury has an independent opinion, and if the probability of a correct decision of a jury member is greater than 0.5, then the probability of a correct jury decision as a whole increases with the number of jury members and tends to one. If, however, the probability of being right for each member of the jury is less than 0.5, then the probability of taking the correct decision by the jury as a whole decreases monotonically and tends to zero with an increase in the number of jurors. <br>\n",
    "$ \\large N $ - number of jurors<br>\n",
    "$ \\large p $ - probability of correct jury decision<br>\n",
    "$ \\large \\mu $ - probability of correct decision of the whole jury<br>\n",
    "$ \\large m $ - $\\large [\\frac{N+1}{2}]$<br>\n",
    "$$ \\large \\mu = \\sum_{i = m}^{N} C_N^ip^i (1-p)^{N-i} $$ \n",
    "If $ \\large p > 0.5 $, then $ \\large \\mu > p $\n",
    "If $ \\large N \\rightarrow \\infty $, then $ \\large \\mu \\rightarrow 1 $\n",
    "\n",
    "Let's look at another example of ensembles - \"The Wisdom of the Crowd\". Frances Galton visited the market in 1906, where a certain lottery for peasants was held.\n",
    "They gathered about 800 people, and they tried to guess the weight of the bull that stood in front of them. The bull weighed 1198 pounds. No peasant guessed the exact weight of the bull, but if we calculate the average of their predictions, we will get 1,177 pounds.\n",
    "This idea of reducing the error was also applied in machine learning.\n",
    "\n",
    "## Bagging\n",
    "\n",
    "Suppose there is a training sample $ \\large X $. Using bootstrap, we generate from it a sample $ \\large X_1, \\dots, X_M $. Now on each sample we will train our classifier $ \\large a_i (x) $. The resulting classifier will average the answers of all these algorithms (in the case of classification, this corresponds to voting): $$ \\large a(x) = \\frac{1}{M}\\sum_{i = 1}^M a_i(x) $$ This scheme can be represented by the picture below.\n",
    "\n",
    "![Image](https://github.com/Yorko/mlcourse_open/blob/master/img/bagging.png?raw=true)\n",
    "\n",
    "### Out-of-bag error\n",
    "\n",
    "Let $ \\large \\ell $ objects in the sample. At each step, all objects fall into the sub-sample with a return equally probable, a single object with probability $ \\large \\frac{1}{\\ell}.$ The probability that the object will NOT fall into the subsample (it was not $ \\large \\ell $ times): $ \\large (1 - \\frac{1}{\\ell})^\\ell$. For $ \\large \\ell \\rightarrow + \\infty $ we obtain the limit $ \\large \\frac{1}{e} $. Then the probability of a specific object falling into the subsample $ \\large \\approx 1 - \\frac{1}{e} \\approx 63\\% $.\n",
    "\n",
    "![image](https://github.com/Yorko/mlcourse_open/blob/master/img/oob.png?raw=true)\n",
    "\n",
    "The figure shows an estimate of the oob error. The upper figure is our initial sample, we divide it into a training sample (on the left) and a test sample (on the right). In the figure on the right, we have a grid of squares that perfectly divides our sample. Now we need to estimate the proportion of correct answers in our test sample. The figure shows that our classifier was mistaken in 4 observations, which we did not use for training. Hence, the fraction of the correct answers of our classifier: $ \\large \\frac{11}{15}*100\\% = 73.33\\% $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_train = pd.read_csv('train.csv', \n",
    "                       dtype={\n",
    "                           'question1': np.str,\n",
    "                           'question2': np.str\n",
    "                       })\n",
    "df_train = df_train[~df_train.question2.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2), max_features=10000)\n",
    "X_train, X_val, y_train, y_val = train_test_split(df_train[[\"question1\", \"question2\"]], df_train.is_duplicate, \n",
    "                                                  stratify=df_train.is_duplicate, random_state=42)\n",
    "questions_train = list(np.concatenate([X_train.question1, X_train.question2]))\n",
    "questions_val = list(np.concatenate([X_val.question1, X_val.question2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions_train_transformed = cv.fit_transform(questions_train)\n",
    "questions_val_transformed = cv.transform(questions_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold_train = int(questions_train_transformed.shape[0] / 2)\n",
    "threshold_val = int(questions_val_transformed.shape[0] / 2)\n",
    "X_train_new = sparse.hstack((questions_train_transformed[:threshold_train], \n",
    "                             questions_train_transformed[threshold_train:]))\n",
    "X_val_new = sparse.hstack((questions_val_transformed[:threshold_val], \n",
    "                           questions_val_transformed[threshold_val:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.901928040725\n"
     ]
    }
   ],
   "source": [
    "dtc = DecisionTreeClassifier(max_depth=20, class_weight='balanced')\n",
    "dtc.fit(X_train_new, y_train)\n",
    "print(log_loss(y_val, dtc.predict_proba(X_val_new)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5789404732\n"
     ]
    }
   ],
   "source": [
    "bg = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=20, class_weight='balanced'), n_estimators=10, \n",
    "                       max_samples=0.7, max_features=0.7, n_jobs=-1, random_state=42)\n",
    "bg.fit(X_train_new, y_train)\n",
    "print(log_loss(y_val, bg.predict_proba(X_val_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved our accuracy only using BaggingClassifier with almost default parameters.\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "Random Forest is an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees (decision trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance).\n",
    "Random decision forests correct for decision trees' habit of overfitting to their training set. <br> \n",
    "\n",
    "So we can say that Random Forest is like bootstrapping algorithm with Decision tree (CART) model. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is called random subspace method or sometimes \"feature bagging\", and it attempts to reduce the correlation between estimators (decision trees) in an ensemble by training them on random samples of features instead of the entire feature set. <br> \n",
    "\n",
    "Main parameters:\n",
    "* n_estimators — the number of trees in the forest (by default=10);\n",
    "* max_features — the number of features to consider when looking for the best split;\n",
    "* criterion — the function to measure the quality of a split ('mse' for regression, gini' or 'entropy' for classification);\n",
    "* min_samples_leaf — the minimum number of samples required to be at a leaf node;\n",
    "* max_depth — the maximum depth of the tree.\n",
    "\n",
    "You could find more information about Randon Forest (and other ensembles) parameters [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.578809468263\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42, max_depth=20, max_features=0.7, class_weight='balanced')\n",
    "rf.fit(X_train_new, y_train)\n",
    "print(log_loss(y_val, rf.predict_proba(X_val_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Pros:\n",
    "* it is one of the most accurate learning algorithms available. For many data sets, it produces a highly accurate classifier;\n",
    "* runs efficiently on large databases;\n",
    "* gives estimates of what variables are important in the classification;\n",
    "* has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing;\n",
    "* has methods for balancing error in class population unbalanced data sets;\n",
    "\n",
    "### Random Forest Cons:\n",
    "* produces a lower accurary on 'sparse' data (e.g. text, bag of words, sparse matrix);\n",
    "* can be overfitted for some datasets with noisy classification/regression tasks;\n",
    "* difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Extremely Randomize Trees\n",
    "\n",
    "or Extra Trees are very similar to Random Forest algorithm, but use differenrt approach to constructing the decion trees:\n",
    "\n",
    "* each tree is built from the complete learning sample (doesn't apply the bagging procedure to construct a set of the training samples for each tree);\n",
    "* for each of the features (randomly selected at each interior node) a discretization threshold (cut-point) is selected at random to define a split, instead of choosing the best cut-point based on the local sample (as in Tree Bagging or in the Random Forests methods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.657443851713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "extra = ExtraTreesClassifier(n_estimators=10, \n",
    "                             max_depth=20,\n",
    "                             random_state=42,\n",
    "                             n_jobs=-1,\n",
    "                             class_weight='balanced',\n",
    "                            )\n",
    "extra.fit(X_train_new, y_train)\n",
    "print(log_loss(y_val, extra.predict_proba(X_val_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage of Extratrees – preventing overfitting. Usually used on last stack layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Algorithms\n",
    "\n",
    "The common ensemble techniques like random forests rely on simple averaging of models in the ensemble. The family of boosting methods is based on a different, constructive strategy of ensemble formation. The main idea of boosting is to add new models to the ensemble sequentially. <br> \n",
    "\n",
    "In gradient boosting  the learning procedure consecutively fits new models to provide a more accurate estimate of the response variable. The principle idea behind this algorithm is to construct the new base-learners to be maximally correlated with the negative gradient of the loss function, associated with the whole ensemble. Simply put, the next algorithm tries to fix the error of the previous one. \n",
    "\n",
    "Briefly consider the constructing of gradient boosting algorithm:\n",
    "\n",
    "* Step 1. Applying a base learning algorithm.\n",
    "* Step 2. Initially all points have same weight (denoted by their size). After the first iteration the points classified correctly are given a lower weight and vice versa.\n",
    "* Step 3. The next algorithm focuses on high weight points and try to classificate them correctly.\n",
    "* Step 4. Iterate steps 2 and 3 till the limit of base learning algorithm is reached or higher accuracy is achieved or no longer improves on an external validation dataset.\n",
    "\n",
    "\n",
    "We will consider XGBoost and LightGbm: gradient boosting frameworks that use tree based learning algorithms. \n",
    "\n",
    "### Boosting parameters\n",
    "Tunning parameters is very important stage, we can signifincantly improve our baseline model. It is very difficult to get answers to practical questions like – Which set of parameters should be tuned ? Let's take a look on general GBm parameters\n",
    "\n",
    "All GBM parameters can be divided in three main groups:\n",
    "1. Tree-Specific Parameters;\n",
    "2. Boosting Parameters;\n",
    "3. Miscellaneous Parameters. \n",
    "\n",
    "\n",
    "The optimal parameters of a model can depend on many scenarios. So it is impossible to create a comprehensive guide for doing so. \n",
    "\n",
    "### XGBoost parameters tuning\n",
    "\n",
    "Usually we start tuning parameters with these first: <br>\n",
    "`n_estimators` - number of boosting rounds, better to use `early_stopping_rounds` <br>\n",
    "`eta` – learning rate, increasing lr reduces convergence time. (usually default value works good)\n",
    "\n",
    "### Control Overfitting\n",
    "When you observe high training accuracy, but low tests accuracy, it is likely that you encounter overfitting problem.\n",
    "\n",
    "There are in general two ways that you can control overfitting:\n",
    "\n",
    "* The first way is to directly control model complexity\n",
    "This include `max_depth` - maximum depth of a tree, increase this value will make the model more complex; <br>\n",
    "`min_child_weight` - minimum sum of instance weight needed in a child; <br>\n",
    "`gamma` - minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "\n",
    "\n",
    "* The second way is to add randomness to make training robust to noise\n",
    "This include `subsample` - subsample ratio of the training instance,\n",
    "`colsample_bytree` - subsample ratio of columns when constructing each tree. <br>\n",
    "\n",
    "\n",
    "### Handle Imbalanced Dataset\n",
    "There are two ways to improve it:\n",
    "\n",
    "* If you care only about the ranking order (AUC) of your prediction\n",
    "Balance the positive and negative weights, via `scale_pos_weight`\n",
    "Use AUC for evaluation\n",
    "* If you care about predicting the right probability\n",
    "In such a case, you cannot re-balance the dataset\n",
    "In such a case, set parameter `max_delta_step` to a finite number (say 1) will help convergence <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = {}\n",
    "param['objective'] = 'binary:logistic'\n",
    "param['max_depth'] = 7\n",
    "param['eta'] = .2\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['subsample'] = 0.7\n",
    "param['silent'] = 1\n",
    "param['eval_metric'] = 'logloss'\n",
    "num_round = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_train = xgb.DMatrix(X_train_new, y_train)\n",
    "xgb_val = xgb.DMatrix(X_val_new, y_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.669942+0.000848789\ttest-logloss:0.670174+0.000868563\n",
      "[1]\ttrain-logloss:0.652754+0.000587322\ttest-logloss:0.653121+0.000563452\n",
      "[2]\ttrain-logloss:0.6404+0.000945502\ttest-logloss:0.641028+0.00102469\n",
      "[3]\ttrain-logloss:0.631624+0.00116385\ttest-logloss:0.63242+0.00123753\n",
      "[4]\ttrain-logloss:0.624356+0.00141998\ttest-logloss:0.625482+0.00164696\n",
      "[5]\ttrain-logloss:0.617642+0.00059521\ttest-logloss:0.619071+0.000964797\n",
      "[6]\ttrain-logloss:0.612043+0.000718684\ttest-logloss:0.613626+0.000983719\n",
      "[7]\ttrain-logloss:0.607767+0.000983182\ttest-logloss:0.60957+0.001202\n",
      "[8]\ttrain-logloss:0.603497+0.00158781\ttest-logloss:0.605523+0.00193997\n",
      "[9]\ttrain-logloss:0.599887+0.00128126\ttest-logloss:0.602043+0.00165774\n",
      "[10]\ttrain-logloss:0.596506+0.00115907\ttest-logloss:0.598875+0.00155805\n",
      "[11]\ttrain-logloss:0.592819+0.000948224\ttest-logloss:0.595491+0.00126061\n",
      "[12]\ttrain-logloss:0.589821+0.000809305\ttest-logloss:0.592602+0.000764039\n",
      "[13]\ttrain-logloss:0.586856+0.00152947\ttest-logloss:0.589915+0.00119879\n",
      "[14]\ttrain-logloss:0.584356+0.000776259\ttest-logloss:0.587596+0.000576152\n",
      "[15]\ttrain-logloss:0.581955+0.00105151\ttest-logloss:0.585412+0.000726372\n",
      "[16]\ttrain-logloss:0.579836+0.000930889\ttest-logloss:0.583374+0.000718036\n",
      "[17]\ttrain-logloss:0.577567+0.00138135\ttest-logloss:0.58126+0.00109529\n",
      "[18]\ttrain-logloss:0.575773+0.00110458\ttest-logloss:0.579649+0.000682609\n",
      "[19]\ttrain-logloss:0.573602+0.00166286\ttest-logloss:0.577653+0.00109078\n",
      "[20]\ttrain-logloss:0.570926+0.0019277\ttest-logloss:0.575188+0.00111916\n",
      "[21]\ttrain-logloss:0.568995+0.00173615\ttest-logloss:0.573435+0.000873373\n",
      "[22]\ttrain-logloss:0.566788+0.00176126\ttest-logloss:0.571425+0.00120214\n",
      "[23]\ttrain-logloss:0.565375+0.00197998\ttest-logloss:0.570163+0.00140893\n",
      "[24]\ttrain-logloss:0.564114+0.00183606\ttest-logloss:0.569031+0.0013002\n",
      "[25]\ttrain-logloss:0.561503+0.00200609\ttest-logloss:0.566607+0.00171645\n",
      "[26]\ttrain-logloss:0.560025+0.00226135\ttest-logloss:0.56525+0.00182731\n",
      "[27]\ttrain-logloss:0.558183+0.00250749\ttest-logloss:0.563547+0.00220771\n",
      "[28]\ttrain-logloss:0.557041+0.00222945\ttest-logloss:0.56253+0.00199316\n",
      "[29]\ttrain-logloss:0.555729+0.00204454\ttest-logloss:0.561349+0.00175767\n",
      "[30]\ttrain-logloss:0.554356+0.00216837\ttest-logloss:0.560135+0.00186398\n",
      "[31]\ttrain-logloss:0.553034+0.00189385\ttest-logloss:0.55894+0.00160513\n",
      "[32]\ttrain-logloss:0.552157+0.00194099\ttest-logloss:0.558193+0.00158437\n",
      "[33]\ttrain-logloss:0.550918+0.00188713\ttest-logloss:0.557039+0.00155038\n",
      "[34]\ttrain-logloss:0.549924+0.00197032\ttest-logloss:0.556174+0.00154906\n",
      "[35]\ttrain-logloss:0.54815+0.00195589\ttest-logloss:0.554502+0.00122099\n",
      "[36]\ttrain-logloss:0.546711+0.00252573\ttest-logloss:0.553189+0.00157768\n",
      "[37]\ttrain-logloss:0.545166+0.00297648\ttest-logloss:0.551826+0.00202831\n",
      "[38]\ttrain-logloss:0.544138+0.00299838\ttest-logloss:0.550944+0.00202167\n",
      "[39]\ttrain-logloss:0.543135+0.00309275\ttest-logloss:0.55009+0.00209367\n",
      "[40]\ttrain-logloss:0.542204+0.00301857\ttest-logloss:0.549283+0.00203995\n",
      "[41]\ttrain-logloss:0.540803+0.00304875\ttest-logloss:0.547983+0.00213479\n",
      "[42]\ttrain-logloss:0.539999+0.00293187\ttest-logloss:0.547278+0.00197849\n",
      "[43]\ttrain-logloss:0.539105+0.00305971\ttest-logloss:0.546476+0.00200929\n",
      "[44]\ttrain-logloss:0.53839+0.00308949\ttest-logloss:0.545859+0.00201996\n",
      "[45]\ttrain-logloss:0.537411+0.00310636\ttest-logloss:0.544998+0.00205206\n",
      "[46]\ttrain-logloss:0.536485+0.00304518\ttest-logloss:0.544226+0.00204706\n",
      "[47]\ttrain-logloss:0.535727+0.00310166\ttest-logloss:0.543613+0.00210312\n",
      "[48]\ttrain-logloss:0.534809+0.00320665\ttest-logloss:0.542831+0.00233259\n",
      "[49]\ttrain-logloss:0.534197+0.00319771\ttest-logloss:0.542288+0.00231509\n"
     ]
    }
   ],
   "source": [
    "cv_results = xgb.cv(param, xgb_train,num_round, nfold=3, stratified=True, seed=42, verbose_eval=1, early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.670007\tval-logloss:0.670375\n",
      "[1]\ttrain-logloss:0.652499\tval-logloss:0.653106\n",
      "[2]\ttrain-logloss:0.639016\tval-logloss:0.639759\n",
      "[3]\ttrain-logloss:0.629368\tval-logloss:0.630127\n",
      "[4]\ttrain-logloss:0.623003\tval-logloss:0.624045\n",
      "[5]\ttrain-logloss:0.616863\tval-logloss:0.617651\n",
      "[6]\ttrain-logloss:0.612027\tval-logloss:0.613004\n",
      "[7]\ttrain-logloss:0.607925\tval-logloss:0.60919\n",
      "[8]\ttrain-logloss:0.603408\tval-logloss:0.604748\n",
      "[9]\ttrain-logloss:0.600034\tval-logloss:0.6015\n",
      "[10]\ttrain-logloss:0.597286\tval-logloss:0.598926\n",
      "[11]\ttrain-logloss:0.594394\tval-logloss:0.596165\n",
      "[12]\ttrain-logloss:0.591621\tval-logloss:0.593557\n",
      "[13]\ttrain-logloss:0.588962\tval-logloss:0.591093\n",
      "[14]\ttrain-logloss:0.586079\tval-logloss:0.58818\n",
      "[15]\ttrain-logloss:0.58283\tval-logloss:0.58516\n",
      "[16]\ttrain-logloss:0.579157\tval-logloss:0.58159\n",
      "[17]\ttrain-logloss:0.576871\tval-logloss:0.579487\n",
      "[18]\ttrain-logloss:0.574985\tval-logloss:0.577661\n",
      "[19]\ttrain-logloss:0.573319\tval-logloss:0.576107\n",
      "[20]\ttrain-logloss:0.571735\tval-logloss:0.57463\n",
      "[21]\ttrain-logloss:0.570322\tval-logloss:0.573379\n",
      "[22]\ttrain-logloss:0.568687\tval-logloss:0.571795\n",
      "[23]\ttrain-logloss:0.566846\tval-logloss:0.570075\n",
      "[24]\ttrain-logloss:0.564954\tval-logloss:0.56818\n",
      "[25]\ttrain-logloss:0.56384\tval-logloss:0.567081\n",
      "[26]\ttrain-logloss:0.56178\tval-logloss:0.56504\n",
      "[27]\ttrain-logloss:0.560346\tval-logloss:0.563736\n",
      "[28]\ttrain-logloss:0.559495\tval-logloss:0.562979\n",
      "[29]\ttrain-logloss:0.556287\tval-logloss:0.559828\n",
      "[30]\ttrain-logloss:0.55515\tval-logloss:0.558708\n",
      "[31]\ttrain-logloss:0.554299\tval-logloss:0.557946\n",
      "[32]\ttrain-logloss:0.552224\tval-logloss:0.55597\n",
      "[33]\ttrain-logloss:0.55094\tval-logloss:0.554768\n",
      "[34]\ttrain-logloss:0.549866\tval-logloss:0.553812\n",
      "[35]\ttrain-logloss:0.549195\tval-logloss:0.553241\n",
      "[36]\ttrain-logloss:0.548166\tval-logloss:0.552296\n",
      "[37]\ttrain-logloss:0.547173\tval-logloss:0.551465\n",
      "[38]\ttrain-logloss:0.545429\tval-logloss:0.549866\n",
      "[39]\ttrain-logloss:0.543943\tval-logloss:0.548534\n",
      "[40]\ttrain-logloss:0.543117\tval-logloss:0.547813\n",
      "[41]\ttrain-logloss:0.54154\tval-logloss:0.546481\n",
      "[42]\ttrain-logloss:0.540503\tval-logloss:0.545517\n",
      "[43]\ttrain-logloss:0.539862\tval-logloss:0.544899\n",
      "[44]\ttrain-logloss:0.538093\tval-logloss:0.543341\n",
      "[45]\ttrain-logloss:0.537414\tval-logloss:0.542752\n",
      "[46]\ttrain-logloss:0.53666\tval-logloss:0.542092\n",
      "[47]\ttrain-logloss:0.536\tval-logloss:0.541487\n",
      "[48]\ttrain-logloss:0.53541\tval-logloss:0.540952\n",
      "[49]\ttrain-logloss:0.53478\tval-logloss:0.540435\n"
     ]
    }
   ],
   "source": [
    "evallist = [(xgb_train, \"train\"), (xgb_val, \"val\")]\n",
    "model_xgboost = xgb.train(param, xgb_train, num_round, evallist, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGbm parameters tuning\n",
    "\n",
    "### For better accuracy\n",
    "\n",
    "* Use large `num_leaves` - number of leaves in one tree (may cause over-fitting) <br>\n",
    "* Use large `max_bin` (max number of bin that feature values will bucket in) <br>\n",
    "* Use small `learning_rate` (shrinkage rate) with large `num_iterations` (only used in prediction task, used to how many trained iterations will be used in prediction) <br>\n",
    "* Use bigger training data <br>\n",
    "\n",
    "\n",
    "### For faster \n",
    "\n",
    "* Use bagging by set `bagging_fraction` (will random select part of data)  and `bagging_freq` (Frequency for bagging) <br>\n",
    "* Use feature sub-sampling by set `feature_fraction` (will random select part of features on each iteration)\n",
    "* Use small `max_bin`\n",
    "* Use `save_binary` (will save the data set(include validation data) to a binary file)  to speed up data loading in future learning <br>\n",
    "* Use parallel learning, refer to [parallel learning guide](https://github.com/Microsoft/LightGBM/wiki/Parallel-Learning-Guide) \n",
    "\n",
    "### Deal with over-fitting\n",
    "\n",
    "* Use small `max_bin` <br>\n",
    "* Use small `num_leaves` <br>\n",
    "* Use `min_data_in_leaf` (minimal number of data in one leaf) <br>\n",
    "* Use bagging by set `bagging_fraction` and `bagging_freq` <br>\n",
    "* Use feature sub-sampling by set `feature_fraction` <br>\n",
    "* Use bigger training data <br>\n",
    "* Try `lambda_l1`, `lambda_l2` and `min_gain_to_split` to regularization <br>\n",
    "* Try `max_depth` to avoid growing deep tree (tree still grows by leaf-wise) <br>\n",
    "\n",
    "To learn more about tuning LightGbm hyperparameters please click [this link](https://github.com/Microsoft/LightGBM/blob/master/docs/Parameters.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's binary_logloss: 0.678982 + 3.59888e-05\n",
      "[2]\tcv_agg's binary_logloss: 0.66741 + 0.000191059\n",
      "[3]\tcv_agg's binary_logloss: 0.656829 + 0.000315858\n",
      "[4]\tcv_agg's binary_logloss: 0.647587 + 0.000388539\n",
      "[5]\tcv_agg's binary_logloss: 0.639934 + 0.000720499\n",
      "[6]\tcv_agg's binary_logloss: 0.633132 + 0.000789629\n",
      "[7]\tcv_agg's binary_logloss: 0.627526 + 0.000862464\n",
      "[8]\tcv_agg's binary_logloss: 0.622271 + 0.000901618\n",
      "[9]\tcv_agg's binary_logloss: 0.617577 + 0.00101382\n",
      "[10]\tcv_agg's binary_logloss: 0.613441 + 0.00101246\n",
      "[11]\tcv_agg's binary_logloss: 0.609706 + 0.000898937\n",
      "[12]\tcv_agg's binary_logloss: 0.606256 + 0.000833069\n",
      "[13]\tcv_agg's binary_logloss: 0.603055 + 0.000580361\n",
      "[14]\tcv_agg's binary_logloss: 0.600184 + 0.000640182\n",
      "[15]\tcv_agg's binary_logloss: 0.597362 + 0.000580897\n",
      "[16]\tcv_agg's binary_logloss: 0.594715 + 0.000663676\n",
      "[17]\tcv_agg's binary_logloss: 0.592269 + 0.000627423\n",
      "[18]\tcv_agg's binary_logloss: 0.59008 + 0.000520252\n",
      "[19]\tcv_agg's binary_logloss: 0.587579 + 0.000453445\n",
      "[20]\tcv_agg's binary_logloss: 0.585517 + 0.000538399\n",
      "[21]\tcv_agg's binary_logloss: 0.583471 + 0.000371544\n",
      "[22]\tcv_agg's binary_logloss: 0.581761 + 0.00029613\n",
      "[23]\tcv_agg's binary_logloss: 0.579998 + 0.000333742\n",
      "[24]\tcv_agg's binary_logloss: 0.578385 + 0.00018162\n",
      "[25]\tcv_agg's binary_logloss: 0.576626 + 0.00029586\n",
      "[26]\tcv_agg's binary_logloss: 0.575185 + 0.000229669\n",
      "[27]\tcv_agg's binary_logloss: 0.573806 + 0.000274997\n",
      "[28]\tcv_agg's binary_logloss: 0.572241 + 0.000255514\n",
      "[29]\tcv_agg's binary_logloss: 0.570863 + 0.00028363\n",
      "[30]\tcv_agg's binary_logloss: 0.569577 + 0.00030053\n",
      "[31]\tcv_agg's binary_logloss: 0.568213 + 0.000335167\n",
      "[32]\tcv_agg's binary_logloss: 0.566982 + 0.000294539\n",
      "[33]\tcv_agg's binary_logloss: 0.565632 + 0.000515649\n",
      "[34]\tcv_agg's binary_logloss: 0.564544 + 0.000515479\n",
      "[35]\tcv_agg's binary_logloss: 0.563423 + 0.000521327\n",
      "[36]\tcv_agg's binary_logloss: 0.562116 + 0.000389391\n",
      "[37]\tcv_agg's binary_logloss: 0.561043 + 0.000292942\n",
      "[38]\tcv_agg's binary_logloss: 0.560013 + 0.00029211\n",
      "[39]\tcv_agg's binary_logloss: 0.559042 + 0.000307634\n",
      "[40]\tcv_agg's binary_logloss: 0.55793 + 0.000255396\n",
      "[41]\tcv_agg's binary_logloss: 0.557004 + 0.000233063\n",
      "[42]\tcv_agg's binary_logloss: 0.556068 + 0.000240829\n",
      "[43]\tcv_agg's binary_logloss: 0.555152 + 0.000318207\n",
      "[44]\tcv_agg's binary_logloss: 0.554124 + 0.00022473\n",
      "[45]\tcv_agg's binary_logloss: 0.55309 + 6.77939e-05\n",
      "[46]\tcv_agg's binary_logloss: 0.552356 + 2.36729e-05\n",
      "[47]\tcv_agg's binary_logloss: 0.551487 + 6.86274e-05\n",
      "[48]\tcv_agg's binary_logloss: 0.550763 + 0.000172911\n",
      "[49]\tcv_agg's binary_logloss: 0.549567 + 0.000364088\n",
      "[50]\tcv_agg's binary_logloss: 0.548843 + 0.000382772\n",
      "[51]\tcv_agg's binary_logloss: 0.547815 + 0.000383863\n",
      "[52]\tcv_agg's binary_logloss: 0.546796 + 0.000146887\n",
      "[53]\tcv_agg's binary_logloss: 0.545833 + 0.000176159\n",
      "[54]\tcv_agg's binary_logloss: 0.54501 + 0.000414098\n",
      "[55]\tcv_agg's binary_logloss: 0.543931 + 0.000101705\n",
      "[56]\tcv_agg's binary_logloss: 0.543249 + 0.000230181\n",
      "[57]\tcv_agg's binary_logloss: 0.542539 + 0.000278254\n",
      "[58]\tcv_agg's binary_logloss: 0.541869 + 0.000228953\n",
      "[59]\tcv_agg's binary_logloss: 0.541245 + 0.00024245\n",
      "[60]\tcv_agg's binary_logloss: 0.54072 + 0.000222167\n",
      "[61]\tcv_agg's binary_logloss: 0.54017 + 0.000219614\n",
      "[62]\tcv_agg's binary_logloss: 0.539647 + 0.000182504\n",
      "[63]\tcv_agg's binary_logloss: 0.538811 + 0.000340615\n",
      "[64]\tcv_agg's binary_logloss: 0.538297 + 0.000325254\n",
      "[65]\tcv_agg's binary_logloss: 0.537739 + 0.000261398\n",
      "[66]\tcv_agg's binary_logloss: 0.537133 + 0.00031086\n",
      "[67]\tcv_agg's binary_logloss: 0.536388 + 0.000288816\n",
      "[68]\tcv_agg's binary_logloss: 0.535888 + 0.000264676\n",
      "[69]\tcv_agg's binary_logloss: 0.535188 + 0.000283163\n",
      "[70]\tcv_agg's binary_logloss: 0.534741 + 0.000318081\n",
      "[71]\tcv_agg's binary_logloss: 0.534289 + 0.000358799\n",
      "[72]\tcv_agg's binary_logloss: 0.533537 + 0.000396196\n",
      "[73]\tcv_agg's binary_logloss: 0.533004 + 0.000443125\n",
      "[74]\tcv_agg's binary_logloss: 0.532403 + 0.000340977\n",
      "[75]\tcv_agg's binary_logloss: 0.53185 + 0.000293269\n",
      "[76]\tcv_agg's binary_logloss: 0.531389 + 0.000258239\n",
      "[77]\tcv_agg's binary_logloss: 0.531031 + 0.000263771\n",
      "[78]\tcv_agg's binary_logloss: 0.530657 + 0.000276389\n",
      "[79]\tcv_agg's binary_logloss: 0.530285 + 0.000266389\n",
      "[80]\tcv_agg's binary_logloss: 0.529891 + 0.000275742\n",
      "[81]\tcv_agg's binary_logloss: 0.529278 + 0.000427234\n",
      "[82]\tcv_agg's binary_logloss: 0.528942 + 0.000418369\n",
      "[83]\tcv_agg's binary_logloss: 0.528575 + 0.000389303\n",
      "[84]\tcv_agg's binary_logloss: 0.528212 + 0.000311821\n",
      "[85]\tcv_agg's binary_logloss: 0.527798 + 0.000341912\n",
      "[86]\tcv_agg's binary_logloss: 0.527139 + 0.000428652\n",
      "[87]\tcv_agg's binary_logloss: 0.526788 + 0.000446264\n",
      "[88]\tcv_agg's binary_logloss: 0.526412 + 0.000408214\n",
      "[89]\tcv_agg's binary_logloss: 0.525736 + 0.000299956\n",
      "[90]\tcv_agg's binary_logloss: 0.525292 + 0.0001622\n",
      "[91]\tcv_agg's binary_logloss: 0.524958 + 9.18187e-05\n",
      "[92]\tcv_agg's binary_logloss: 0.524541 + 6.61519e-05\n",
      "[93]\tcv_agg's binary_logloss: 0.524178 + 0.000115389\n",
      "[94]\tcv_agg's binary_logloss: 0.523872 + 8.68618e-05\n",
      "[95]\tcv_agg's binary_logloss: 0.52355 + 9.1881e-05\n",
      "[96]\tcv_agg's binary_logloss: 0.523197 + 0.000149956\n",
      "[97]\tcv_agg's binary_logloss: 0.522878 + 0.000181536\n",
      "[98]\tcv_agg's binary_logloss: 0.522501 + 0.000296142\n",
      "[99]\tcv_agg's binary_logloss: 0.521924 + 0.00033534\n",
      "[100]\tcv_agg's binary_logloss: 0.521507 + 0.000409049\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {\n",
    "    'application':'binary',\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': 20,\n",
    "    'feature_fraction': 0.7,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'max_bin': 200,\n",
    "    'metric': 'binary_logloss',\n",
    "    'verbose': 1\n",
    "    \n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train_new.tocsc().astype(\"float32\"), y_train)\n",
    "lgb_val = lgb.Dataset(X_val_new.tocsc().astype(\"float32\"), y_val)\n",
    "results = lightgbm.cv(params, lgb_train, 100, nfold=3, stratified=True, early_stopping_rounds=5, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's binary_logloss: 0.679\n",
      "[2]\tvalid_0's binary_logloss: 0.667174\n",
      "[3]\tvalid_0's binary_logloss: 0.656984\n",
      "[4]\tvalid_0's binary_logloss: 0.647965\n",
      "[5]\tvalid_0's binary_logloss: 0.640312\n",
      "[6]\tvalid_0's binary_logloss: 0.633768\n",
      "[7]\tvalid_0's binary_logloss: 0.627799\n",
      "[8]\tvalid_0's binary_logloss: 0.622441\n",
      "[9]\tvalid_0's binary_logloss: 0.61774\n",
      "[10]\tvalid_0's binary_logloss: 0.613628\n",
      "[11]\tvalid_0's binary_logloss: 0.609851\n",
      "[12]\tvalid_0's binary_logloss: 0.606365\n",
      "[13]\tvalid_0's binary_logloss: 0.603248\n",
      "[14]\tvalid_0's binary_logloss: 0.600255\n",
      "[15]\tvalid_0's binary_logloss: 0.59725\n",
      "[16]\tvalid_0's binary_logloss: 0.593997\n",
      "[17]\tvalid_0's binary_logloss: 0.591503\n",
      "[18]\tvalid_0's binary_logloss: 0.589416\n",
      "[19]\tvalid_0's binary_logloss: 0.587224\n",
      "[20]\tvalid_0's binary_logloss: 0.585438\n",
      "[21]\tvalid_0's binary_logloss: 0.583612\n",
      "[22]\tvalid_0's binary_logloss: 0.581918\n",
      "[23]\tvalid_0's binary_logloss: 0.58024\n",
      "[24]\tvalid_0's binary_logloss: 0.578357\n",
      "[25]\tvalid_0's binary_logloss: 0.576895\n",
      "[26]\tvalid_0's binary_logloss: 0.575308\n",
      "[27]\tvalid_0's binary_logloss: 0.573954\n",
      "[28]\tvalid_0's binary_logloss: 0.572318\n",
      "[29]\tvalid_0's binary_logloss: 0.570907\n",
      "[30]\tvalid_0's binary_logloss: 0.569653\n",
      "[31]\tvalid_0's binary_logloss: 0.568285\n",
      "[32]\tvalid_0's binary_logloss: 0.566958\n",
      "[33]\tvalid_0's binary_logloss: 0.565745\n",
      "[34]\tvalid_0's binary_logloss: 0.564307\n",
      "[35]\tvalid_0's binary_logloss: 0.563291\n",
      "[36]\tvalid_0's binary_logloss: 0.562183\n",
      "[37]\tvalid_0's binary_logloss: 0.560917\n",
      "[38]\tvalid_0's binary_logloss: 0.559832\n",
      "[39]\tvalid_0's binary_logloss: 0.558136\n",
      "[40]\tvalid_0's binary_logloss: 0.557302\n",
      "[41]\tvalid_0's binary_logloss: 0.556285\n",
      "[42]\tvalid_0's binary_logloss: 0.555125\n",
      "[43]\tvalid_0's binary_logloss: 0.554326\n",
      "[44]\tvalid_0's binary_logloss: 0.553601\n",
      "[45]\tvalid_0's binary_logloss: 0.552674\n",
      "[46]\tvalid_0's binary_logloss: 0.551717\n",
      "[47]\tvalid_0's binary_logloss: 0.550772\n",
      "[48]\tvalid_0's binary_logloss: 0.549968\n",
      "[49]\tvalid_0's binary_logloss: 0.548419\n",
      "[50]\tvalid_0's binary_logloss: 0.547685\n",
      "[51]\tvalid_0's binary_logloss: 0.546274\n",
      "[52]\tvalid_0's binary_logloss: 0.545514\n",
      "[53]\tvalid_0's binary_logloss: 0.544883\n",
      "[54]\tvalid_0's binary_logloss: 0.544111\n",
      "[55]\tvalid_0's binary_logloss: 0.54325\n",
      "[56]\tvalid_0's binary_logloss: 0.542606\n",
      "[57]\tvalid_0's binary_logloss: 0.541943\n",
      "[58]\tvalid_0's binary_logloss: 0.541427\n",
      "[59]\tvalid_0's binary_logloss: 0.54088\n",
      "[60]\tvalid_0's binary_logloss: 0.540216\n",
      "[61]\tvalid_0's binary_logloss: 0.53949\n",
      "[62]\tvalid_0's binary_logloss: 0.538928\n",
      "[63]\tvalid_0's binary_logloss: 0.538359\n",
      "[64]\tvalid_0's binary_logloss: 0.537854\n",
      "[65]\tvalid_0's binary_logloss: 0.537346\n",
      "[66]\tvalid_0's binary_logloss: 0.536681\n",
      "[67]\tvalid_0's binary_logloss: 0.536053\n",
      "[68]\tvalid_0's binary_logloss: 0.535602\n",
      "[69]\tvalid_0's binary_logloss: 0.535184\n",
      "[70]\tvalid_0's binary_logloss: 0.534582\n",
      "[71]\tvalid_0's binary_logloss: 0.534081\n",
      "[72]\tvalid_0's binary_logloss: 0.533553\n",
      "[73]\tvalid_0's binary_logloss: 0.533079\n",
      "[74]\tvalid_0's binary_logloss: 0.53274\n",
      "[75]\tvalid_0's binary_logloss: 0.532168\n",
      "[76]\tvalid_0's binary_logloss: 0.531636\n",
      "[77]\tvalid_0's binary_logloss: 0.531167\n",
      "[78]\tvalid_0's binary_logloss: 0.530655\n",
      "[79]\tvalid_0's binary_logloss: 0.53019\n",
      "[80]\tvalid_0's binary_logloss: 0.529622\n",
      "[81]\tvalid_0's binary_logloss: 0.529076\n",
      "[82]\tvalid_0's binary_logloss: 0.528654\n",
      "[83]\tvalid_0's binary_logloss: 0.528329\n",
      "[84]\tvalid_0's binary_logloss: 0.527754\n",
      "[85]\tvalid_0's binary_logloss: 0.527136\n",
      "[86]\tvalid_0's binary_logloss: 0.526781\n",
      "[87]\tvalid_0's binary_logloss: 0.526351\n",
      "[88]\tvalid_0's binary_logloss: 0.526018\n",
      "[89]\tvalid_0's binary_logloss: 0.525664\n",
      "[90]\tvalid_0's binary_logloss: 0.525289\n",
      "[91]\tvalid_0's binary_logloss: 0.524915\n",
      "[92]\tvalid_0's binary_logloss: 0.524589\n",
      "[93]\tvalid_0's binary_logloss: 0.524171\n",
      "[94]\tvalid_0's binary_logloss: 0.523836\n",
      "[95]\tvalid_0's binary_logloss: 0.522896\n",
      "[96]\tvalid_0's binary_logloss: 0.522613\n",
      "[97]\tvalid_0's binary_logloss: 0.5223\n",
      "[98]\tvalid_0's binary_logloss: 0.522083\n",
      "[99]\tvalid_0's binary_logloss: 0.521828\n",
      "[100]\tvalid_0's binary_logloss: 0.521506\n"
     ]
    }
   ],
   "source": [
    "model_lgm = lightgbm.train(params, lgb_train, 100, valid_sets=lgb_val, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricks\n",
    "\n",
    "### Simple average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.508149219898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg = LogisticRegression(n_jobs=-1)\n",
    "lg.fit(X_train_new, y_train)\n",
    "lg_preds = lg.predict_proba(X_val_new)\n",
    "print(log_loss(y_val, lg_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Booster.__del__ of <xgboost.core.Booster object at 0x129765860>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\", line 669, in __del__\n",
      "    _LIB.XGBoosterFree(self.handle)\n",
      "AttributeError: 'Booster' object has no attribute 'handle'\n",
      "Exception ignored in: <bound method Booster.__del__ of <xgboost.core.Booster object at 0x1229b9198>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/site-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\", line 669, in __del__\n",
      "    _LIB.XGBoosterFree(self.handle)\n",
      "AttributeError: 'Booster' object has no attribute 'handle'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521505699925\n"
     ]
    }
   ],
   "source": [
    "lgbm_preds = model_lgm.predict(X_val_new.tocsc().astype(\"float32\"))\n",
    "print(log_loss(y_val, lgbm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.490515057107\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_val, (lgbm_preds + lg_preds[:,1]) / 2.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.488606105114\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_val, lgbm_preds*0.35 + lg_preds[:,1] * 0.65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498213465692\n"
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(n_jobs=-1, C=0.1)\n",
    "lg.fit(X_train_new, y_train)\n",
    "lg_preds_2 = lg.predict_proba(X_val_new)\n",
    "print(log_loss(y_val, lg_preds_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.499804060129\n"
     ]
    }
   ],
   "source": [
    "print(log_loss(y_val, (lg_preds_2[:,1] + lg_preds[:,1]) / 2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.98043019,  0.75241795],\n",
       "       [ 0.98043019,  1.        ,  0.81114385],\n",
       "       [ 0.75241795,  0.81114385,  1.        ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef([lg_preds[:,1], lg_preds_2[:,1], lgbm_preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = pd.read_csv(\"/Users/vitaliyradchenko/Downloads/xgb_seed12357_n315.csv\")\n",
    "test2 = pd.read_csv(\"/Users/vitaliyradchenko/Downloads/xgb_seed12357_n480.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1[\"is_duplicate\"] = test1[\"is_duplicate\"]*0.55 + test2[\"is_duplicate\"]*0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1.to_csv(\"submit.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending\n",
    "\n",
    "![img](https://alexanderdyakonov.files.wordpress.com/2017/03/stacking.png?w=1400)\n",
    "\n",
    "### Stacking\n",
    "\n",
    "![img](https://alexanderdyakonov.files.wordpress.com/2017/03/stacking-2b.png?w=1400)\n",
    "\n",
    "Further reading: https://alexanderdyakonov.wordpress.com/2017/03/10/cтекинг-stacking-и-блендинг-blending/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
